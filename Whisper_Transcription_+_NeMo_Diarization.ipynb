{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":64415,"status":"ok","timestamp":1678241558614,"user":{"displayName":"董柏葳","userId":"11007664106278210356"},"user_tz":-480},"id":"___mJb9Z9YZX","outputId":"f6447182-aca9-4313-d6a1-87fa13875ac1"},"outputs":[],"source":["# mount\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"eCmjcOc9yEtQ"},"source":["# Installing Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":82594,"status":"ok","timestamp":1678114455451,"user":{"displayName":"董柏葳","userId":"11007664106278210356"},"user_tz":-480},"id":"Tn1c-CoDv2kw","outputId":"516a5bf5-c2f1-48cb-82eb-94d232dd5d37"},"outputs":[],"source":["!pip install torch==1.13.1+cu116 torchaudio==0.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n","\n","!pip install torchtext==0.14.1\n","\n","\n","!pip install nemo_toolkit[asr]==1.15.0\n","!pip install git+https://github.com/openai/whisper.git\n","!pip install git+https://github.com/m-bain/whisperX.git@4cb167a225c0ebaea127fd6049abfaa3af9f8bb4\n","!pip install git+https://github.com/facebookresearch/demucs#egg=demucs\n","!pip install deepmultilingualpunctuation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35195,"status":"ok","timestamp":1678114490635,"user":{"displayName":"董柏葳","userId":"11007664106278210356"},"user_tz":-480},"id":"5q0BFjAVzKuo","outputId":"dc910112-9cfd-44cc-cc13-7ff058bf6084"},"outputs":[],"source":["!pip install pytorch-lightning==1.8.6\n","!pip install transformers==4.26.1"]},{"cell_type":"markdown","metadata":{"id":"2ZhAsVIvGevq"},"source":["# Import"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31108,"status":"ok","timestamp":1678114521740,"user":{"displayName":"董柏葳","userId":"11007664106278210356"},"user_tz":-480},"id":"YzhncHP0ytbQ","outputId":"19e378bc-3ee1-46d8-b5a8-9c7202d21edc"},"outputs":[],"source":["import os\n","import wget\n","from omegaconf import OmegaConf\n","import json\n","import shutil\n","from whisper import load_model\n","import whisperx\n","import torch\n","import librosa\n","import soundfile\n","from nemo.collections.asr.models.msdd_models import NeuralDiarizer\n","from deepmultilingualpunctuation import PunctuationModel\n","import re"]},{"cell_type":"markdown","metadata":{"id":"jbsUt3SwyhjD"},"source":["# Helper Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1678114521740,"user":{"displayName":"董柏葳","userId":"11007664106278210356"},"user_tz":-480},"id":"Se6Hc7CZygxu","outputId":"9d36781b-6bd8-423a-b1ce-4b866b39e773"},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 9.18 ms (started: 2023-03-06 14:55:19 +00:00)\n"]}],"source":["def create_config():\n","    data_dir = \"./\"\n","    DOMAIN_TYPE = \"telephonic\"  # Can be meeting or telephonic based on domain type of the audio file\n","    CONFIG_FILE_NAME = f\"diar_infer_{DOMAIN_TYPE}.yaml\"\n","    CONFIG_URL = f\"https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/speaker_tasks/diarization/conf/inference/{CONFIG_FILE_NAME}\"\n","    MODEL_CONFIG = os.path.join(data_dir, CONFIG_FILE_NAME)\n","    if not os.path.exists(MODEL_CONFIG):\n","        MODEL_CONFIG = wget.download(CONFIG_URL, data_dir)\n","\n","    config = OmegaConf.load(MODEL_CONFIG)\n","\n","    ROOT = os.getcwd()\n","    data_dir = os.path.join(ROOT, \"data\")\n","    os.makedirs(data_dir, exist_ok=True)\n","\n","    meta = {\n","        \"audio_filepath\": \"mono_file.wav\",\n","        \"offset\": 0,\n","        \"duration\": None,\n","        \"label\": \"infer\",\n","        \"text\": \"-\",\n","        \"rttm_filepath\": None,\n","        \"uem_filepath\": None,\n","    }\n","    with open(\"data/input_manifest.json\", \"w\") as fp:\n","        json.dump(meta, fp)\n","        fp.write(\"\\n\")\n","\n","    pretrained_vad = \"vad_multilingual_marblenet\"\n","    pretrained_speaker_model = \"titanet_large\"\n","\n","    config.num_workers = 1  # Workaround for multiprocessing hanging with ipython issue\n","\n","    output_dir = \"nemo_outputs\"  # os.path.join(ROOT, 'outputs')\n","    os.makedirs(output_dir, exist_ok=True)\n","    config.diarizer.manifest_filepath = \"data/input_manifest.json\"\n","    config.diarizer.out_dir = (\n","        output_dir  # Directory to store intermediate files and prediction outputs\n","    )\n","\n","    config.diarizer.speaker_embeddings.model_path = pretrained_speaker_model\n","    config.diarizer.oracle_vad = (\n","        False  # compute VAD provided with model_path to vad config\n","    )\n","    config.diarizer.clustering.parameters.oracle_num_speakers = False\n","\n","    # Here, we use our in-house pretrained NeMo VAD model\n","    config.diarizer.vad.model_path = pretrained_vad\n","    config.diarizer.vad.parameters.onset = 0.8\n","    config.diarizer.vad.parameters.offset = 0.6\n","    config.diarizer.vad.parameters.pad_offset = -0.05\n","    config.diarizer.msdd_model.model_path = (\n","        \"diar_msdd_telephonic\"  # Telephonic speaker diarization model\n","    )\n","\n","    return config\n","\n","\n","def get_word_ts_anchor(s, e, option=\"start\"):\n","    if option == \"end\":\n","        return e\n","    elif option == \"mid\":\n","        return (s + e) / 2\n","    return s\n","\n","\n","def get_words_speaker_mapping(wrd_ts, spk_ts, word_anchor_option=\"start\"):\n","    s, e, sp = spk_ts[0]\n","    wrd_pos, turn_idx = 0, 0\n","    wrd_spk_mapping = []\n","    for wrd_dict in wrd_ts:\n","        ws, we, wrd = (\n","            int(wrd_dict[\"start\"] * 1000),\n","            int(wrd_dict[\"end\"] * 1000),\n","            wrd_dict[\"text\"],\n","        )\n","        wrd_pos = get_word_ts_anchor(ws, we, word_anchor_option)\n","        while wrd_pos > float(e):\n","            turn_idx += 1\n","            turn_idx = min(turn_idx, len(spk_ts) - 1)\n","            s, e, sp = spk_ts[turn_idx]\n","            if turn_idx == len(spk_ts) - 1:\n","                e = get_word_ts_anchor(ws, we, option=\"end\")\n","        wrd_spk_mapping.append(\n","            {\"word\": wrd, \"start_time\": ws, \"end_time\": we, \"speaker\": sp}\n","        )\n","    return wrd_spk_mapping\n","\n","\n","def get_sentences_speaker_mapping(word_speaker_mapping, spk_ts):\n","    s, e, spk = spk_ts[0]\n","    prev_spk = spk\n","\n","    snts = []\n","    snt = {\"speaker\": f\"Speaker {spk}\", \"start_time\": s, \"end_time\": e, \"text\": \"\"}\n","\n","    for wrd_dict in word_speaker_mapping:\n","        wrd, spk = wrd_dict[\"word\"], wrd_dict[\"speaker\"]\n","        s, e = wrd_dict[\"start_time\"], wrd_dict[\"end_time\"]\n","        if spk != prev_spk:\n","            snts.append(snt)\n","            snt = {\n","                \"speaker\": f\"Speaker {spk}\",\n","                \"start_time\": s,\n","                \"end_time\": e,\n","                \"text\": \"\",\n","            }\n","        else:\n","            snt[\"end_time\"] = e\n","        snt[\"text\"] += wrd + \" \"\n","        prev_spk = spk\n","\n","    snts.append(snt)\n","    return snts\n","\n","\n","def get_speaker_aware_transcript(sentences_speaker_mapping, f):\n","    for sentence_dict in sentences_speaker_mapping:\n","        sp = sentence_dict[\"speaker\"]\n","        text = sentence_dict[\"text\"]\n","        f.write(f\"\\n\\n{sp}: {text}\")\n","\n","\n","def format_timestamp(\n","    milliseconds: float, always_include_hours: bool = False, decimal_marker: str = \".\"\n","):\n","    assert milliseconds >= 0, \"non-negative timestamp expected\"\n","\n","    hours = milliseconds // 3_600_000\n","    milliseconds -= hours * 3_600_000\n","\n","    minutes = milliseconds // 60_000\n","    milliseconds -= minutes * 60_000\n","\n","    seconds = milliseconds // 1_000\n","    milliseconds -= seconds * 1_000\n","\n","    hours_marker = f\"{hours:02d}:\" if always_include_hours or hours > 0 else \"\"\n","    return (\n","        f\"{hours_marker}{minutes:02d}:{seconds:02d}{decimal_marker}{milliseconds:03d}\"\n","    )\n","\n","\n","def write_srt(transcript, file):\n","    \"\"\"\n","    Write a transcript to a file in SRT format.\n","    \"\"\"\n","    for i, segment in enumerate(transcript, start=1):\n","        # write srt lines\n","        print(\n","            f\"{i}\\n\"\n","            f\"{format_timestamp(segment['start_time'], always_include_hours=True, decimal_marker=',')} --> \"\n","            f\"{format_timestamp(segment['end_time'], always_include_hours=True, decimal_marker=',')}\\n\"\n","            f\"{segment['speaker']}: {segment['text'].strip().replace('-->', '->')}\\n\",\n","            file=file,\n","            flush=True,\n","        )\n","\n","\n","def cleanup(path: str):\n","    \"\"\"path could either be relative or absolute.\"\"\"\n","    # check if file or directory exists\n","    if os.path.isfile(path) or os.path.islink(path):\n","        # remove file\n","        os.remove(path)\n","    elif os.path.isdir(path):\n","        # remove directory and all its content\n","        shutil.rmtree(path)\n","    else:\n","        raise ValueError(\"Path {} is not a file or dir.\".format(path))"]},{"cell_type":"markdown","metadata":{"id":"B7qWQb--1Xcw"},"source":["# Options"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1678114521740,"user":{"displayName":"董柏葳","userId":"11007664106278210356"},"user_tz":-480},"id":"ONlFrSnD0FOp","outputId":"602bd64f-d822-455c-8570-9b43d103870f"},"outputs":[],"source":["# Name of the audio file (relative path e.g. 'xx.mp3' or 'some_dir/xx.mp3')\n","audio_path = '/content/drive/MyDrive/whisper-test/audio.m4a'\n","\n","# Whether to enable music removal from speech, helps increase diarization quality but uses alot of ram\n","enable_stemming = True\n","\n","# (choose from 'tiny.en', 'tiny', 'base.en', 'base', 'small.en', 'small', 'medium.en', 'medium', 'large-v1', 'large-v2', 'large')\n","# only large-* can output chinese transcription!\n","whisper_model_name = 'large-v2'\n","\n","root_dir_path = os.path.dirname(os.path.abspath(audio_path))\n","audio_file_name = os.path.split(os.path.abspath(audio_path))[-1]\n","print(root_dir_path, audio_file_name)"]},{"cell_type":"markdown","metadata":{"id":"eGRkJkt8Gevt"},"source":["# Main"]},{"cell_type":"markdown","metadata":{"id":"7ZS4xXmE2NGP"},"source":["## Preprocess: Separating music from speech using Demucs\n","\n","---\n","\n","By isolating the vocals from the rest of the audio, it becomes easier to identify and track individual speakers based on the spectral and temporal characteristics of their speech signals. Source separation is just one of many techniques that can be used as a preprocessing step to help improve the accuracy and reliability of the overall diarization process."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39854,"status":"ok","timestamp":1678114561591,"user":{"displayName":"董柏葳","userId":"11007664106278210356"},"user_tz":-480},"id":"HKcgQUrAzsJZ","outputId":"544521e3-ce3f-4a08-fc39-3443b3ca3042"},"outputs":[],"source":["if enable_stemming:\n","    # Isolate vocals from the rest of the audio\n","\n","    temp_path = os.path.join(root_dir_path, \"temp_outputs\")\n","    return_code = os.system(\n","        f'python3 -m demucs.separate -n htdemucs_ft --two-stems=vocals \"{audio_path}\" -o \"{temp_path}\" -d cpu'\n","    )\n","\n","    if return_code != 0:\n","        print(\n","            \"Source splitting failed, using original audio file.\"\n","        )\n","        vocal_target = audio_path\n","    else:\n","        vocal_target = f\"{temp_path}/htdemucs_ft/{audio_file_name[:-4]}/vocals.wav\"\n","else:\n","    vocal_target = audio_path"]},{"cell_type":"markdown","metadata":{"id":"UYg9VWb22Tz8"},"source":["## Transcriping audio using Whisper and realligning timestamps using Wav2Vec2\n","---\n","This code uses two different open-source models to transcribe speech and perform forced alignment on the resulting transcription.\n","\n","The first model is called OpenAI Whisper, which is a speech recognition model that can transcribe speech with high accuracy. The code loads the whisper model and uses it to transcribe the vocal_target file.\n","\n","The output of the transcription process is a set of text segments with corresponding timestamps indicating when each segment was spoken.\n","\n","The second model used is called wav2vec2, which is a large-scale neural network that is designed to learn representations of speech that are useful for a variety of speech processing tasks, including speech recognition and alignment.\n","\n","The code loads the wav2vec2 alignment model and uses it to align the transcription segments with the original audio signal contained in the vocal_target file. This process involves finding the exact timestamps in the audio signal where each segment was spoken and aligning the text accordingly.\n","\n","By combining the outputs of the two models, the code produces a fully aligned transcription of the speech contained in the vocal_target file. This aligned transcription can be useful for a variety of speech processing tasks, such as speaker diarization, sentiment analysis, and language identification."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5341907,"status":"ok","timestamp":1678119903956,"user":{"displayName":"董柏葳","userId":"11007664106278210356"},"user_tz":-480},"id":"5-VKFn530oTl","outputId":"7b20dbf1-914d-47b1-c5f9-b180c214e785"},"outputs":[],"source":["# Large models result in considerably better and more aligned (words, timestamps) mapping.\n","whisper_model = load_model(whisper_model_name)\n","whisper_results = whisper_model.transcribe(vocal_target, verbose=True, language = \"zh\", initial_prompt=\"以下是繁體中文的句子\")\n","\n","# clear gpu vram\n","del whisper_model\n","torch.cuda.empty_cache()\n","\n","device = \"cpu\"\n","alignment_model, metadata = whisperx.load_align_model(\n","    language_code=whisper_results[\"language\"], device=device\n",")\n","result_aligned = whisperx.align(\n","    whisper_results[\"segments\"], alignment_model, metadata, vocal_target, device\n",")\n","\n","# clear gpu vram\n","del alignment_model\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{"id":"D1gkViCf2-CV"},"source":["## Speaker Diarization using NeMo MSDD Model\n","---\n","This code uses a model called Nvidia NeMo MSDD (Multi-scale Diarization Decoder) to perform speaker diarization on an audio signal. Speaker diarization is the process of separating an audio signal into different segments based on who is speaking at any given time."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25167,"status":"ok","timestamp":1678119929110,"user":{"displayName":"董柏葳","userId":"11007664106278210356"},"user_tz":-480},"id":"m6l35qQNGevv","outputId":"f379ba30-1d97-4301-9a73-ce85c610ea0c"},"outputs":[],"source":["# Convert audio to mono for NeMo combatibility\n","\n","signal, sample_rate = librosa.load(vocal_target, sr=None)\n","temp_path = os.path.join(root_dir_path, \"temp_outputs\")\n","if not os.path.exists(temp_path):\n","    os.mkdir(temp_path)\n","os.chdir(temp_path)\n","soundfile.write(\"mono_file.wav\", signal, sample_rate, \"PCM_24\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":817328,"status":"ok","timestamp":1678120746424,"user":{"displayName":"董柏葳","userId":"11007664106278210356"},"user_tz":-480},"id":"C7jIpBCH02RL","outputId":"26cb532d-d332-4676-8659-0ad1ba7d720e"},"outputs":[],"source":["# Initialize NeMo MSDD diarization model\n","msdd_model = NeuralDiarizer(cfg=create_config())\n","msdd_model.diarize()\n","\n","del msdd_model\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{"id":"NmkZYaDAEOAg"},"source":["## Mapping Spekers to Sentences According to Timestamps"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1678120746425,"user":{"displayName":"董柏葳","userId":"11007664106278210356"},"user_tz":-480},"id":"E65LUGQe02zw","outputId":"d0dc8a40-fb55-4f3e-f7ba-91705573ee8a"},"outputs":[],"source":["# Reading timestamps <> Speaker Labels mapping\n","\n","output_dir = \"nemo_outputs\"\n","\n","speaker_ts = []\n","with open(f\"{output_dir}/pred_rttms/mono_file.rttm\", \"r\") as f:\n","    lines = f.readlines()\n","    for line in lines:\n","        line_list = line.split(\" \")\n","        s = int(float(line_list[5]) * 1000)\n","        e = s + int(float(line_list[8]) * 1000)\n","        speaker_ts.append([s, e, int(line_list[11].split(\"_\")[-1])])\n","\n","wsm = get_words_speaker_mapping(result_aligned[\"segments\"], speaker_ts, \"start\")\n","ssm = get_sentences_speaker_mapping(wsm, speaker_ts)"]},{"cell_type":"markdown","metadata":{"id":"vF2QAtLOFvwZ"},"source":["## Cleanup and Exporing the results"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":704,"status":"ok","timestamp":1678120747126,"user":{"displayName":"董柏葳","userId":"11007664106278210356"},"user_tz":-480},"id":"kFTyKI6B1MI0","outputId":"2ec01308-3ea2-41bd-fb4f-c3ba85fca792"},"outputs":[],"source":["os.chdir(root_dir_path)  # back to parent dir\n","with open(f\"{audio_path[:-4]}.txt\", \"w\", encoding=\"utf-8-sig\") as f:\n","    get_speaker_aware_transcript(ssm, f)\n","\n","with open(f\"{audio_path[:-4]}.srt\", \"w\", encoding=\"utf-8-sig\") as srt:\n","    write_srt(ssm, srt)\n","\n","cleanup(temp_path)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["eCmjcOc9yEtQ","jbsUt3SwyhjD"],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
